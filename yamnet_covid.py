# -*- coding: utf-8 -*-
"""YAMNet covid + online_augm + EffNet

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_y-UqotHXHIh1BQwvIKfSWTQjEr5eDej

Etraxt audio features with YAMNet (https://github.com/tensorflow/models/tree/master/research/audioset/yamnet) and train a classifier.

YAMNet extracts "frames" from the audio signal and processes batches of these frames. This version of the model uses frames that are 0.96 second long and extracts one frame every 0.48 seconds.

The model accepts a 1-D float32 Tensor or NumPy array containing a waveform of arbitrary length, represented as single-channel (mono) 16 kHz samples in the range [-1.0, +1.0]. The model returns 3 outputs, including the class scores, embeddings (which you will use for transfer learning), and the log mel spectrogram. In this work the embeddings are used to train a classifier.


When extracting embeddings from the WAV data, you get an array of shape (N, 1024) where N is the number of frames that YAMNet found (one for every 0.48 seconds of audio).
"""

'''
!pip install tensorflow_io
!apt install ffmpeg
!pip install audiomentations
'''

from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift, AddShortNoises
import tensorflow as tf
from tensorflow.keras import layers
import numpy as np
import tensorflow_io as tfio
from tensorflow.keras.metrics import categorical_crossentropy
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.metrics import confusion_matrix
import itertools
import os
import shutil
import random
import glob
import matplotlib.pyplot as plt
import warnings
import scipy.io
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import Model
from tensorflow.keras.layers import concatenate, Input, Activation, Dense, Dropout, Conv2D, Flatten, MaxPooling2D, \
  GlobalMaxPooling2D, GlobalAveragePooling1D, AveragePooling2D, Input, Add, BatchNormalization
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler, TensorBoard, ReduceLROnPlateau
import pathlib
import tensorflow_hub as hub
import pandas as pd
import warnings
from tensorflow.keras.layers import TimeDistributed, LayerNormalization
from tensorflow.keras.regularizers import l2
from sklearn.utils import shuffle
from numpy import load
from tensorflow.python.client import device_lib
from sklearn.metrics import roc_curve
from sklearn.metrics import auc


warnings.filterwarnings('ignore')

print(tf.config.list_physical_devices())
print(device_lib.list_local_devices())
print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))
"""Create training, validation and test batches. Data augmentation is exploited for training and validation sets.

Download and test YAMNet
"""

yamnet_model_handle = 'https://tfhub.dev/google/yamnet/1'
yamnet_model = hub.load(yamnet_model_handle)


# Utility functions for loading audio files and making sure the sample rate is correct.


@tf.function
def load_wav_16k_mono(filename):
  """ Load a WAV file, convert it to a float tensor, resample to 16 kHz single-channel audio. """
  file_contents = tf.io.read_file(filename)
  wav, sample_rate = tf.audio.decode_wav(
    file_contents,
    desired_channels=1)
  wav = tf.squeeze(wav, axis=-1)
  sample_rate = tf.cast(sample_rate, dtype=tf.int64)

  wav = tfio.audio.resample(wav, rate_in=sample_rate, rate_out=16000)

  return wav


# Load csv with metadata
base_dir = 'segmented_wav_audio_4_max096'
cdf_train = pd.read_csv(os.path.join(base_dir, 'Train_seg_09_data.csv'))
cdf_val = pd.read_csv(os.path.join(base_dir, 'Val_seg_09_data.csv'))
cdf_test = pd.read_csv(os.path.join(base_dir, 'Test_seg_09_data.csv'))

my_classes = ['positive', 'negative']

# Load values
emb_mean = load('embeddings_mean.npy')  # em
emb_std = load('embeddings_std.npy')
max_spec = load('spectrogrmas_abs_max.npy')
max_mfccs = load('mfccs_abs_max.npy')

# Shuffle dataset
RAND_SEED = 10


def shuffle_my_ds(ds):
  ds_shuffled = shuffle(ds, random_state=RAND_SEED)
  ds_shuffled.reset_index(inplace=True, drop=True)
  return ds_shuffled


cdf_train = shuffle_my_ds(cdf_train)
cdf_val = shuffle_my_ds(cdf_val)


def partition_info(df):
  # Get info on number of positive and negative cases
  lab = np.array(df['target'].tolist())
  total_cases = len(lab)
  pos_num = sum(lab)
  negative_num = total_cases - pos_num

  print('Negative cases = ', negative_num)
  print('Positive cases =', pos_num)
  print('P/N = ', pos_num / negative_num, '\n')

  return negative_num, pos_num


print('Train')
neg, pos = partition_info(cdf_train)
print('Val')
partition_info(cdf_val)
print('Test')
partition_info(cdf_test)

# Scaling by total/2 helps keep the loss to a similar magnitude.
# The sum of the weights of all examples stays the same.
total = neg + pos
weight_for_0 = (1 / neg) * (total) / 2.0
weight_for_1 = (1 / pos) * (total) / 2.0

class_weight = {0: weight_for_0, 1: weight_for_1}

print('Weight for Negative class: {:.2f}'.format(weight_for_0))
print('Weight for Positive class: {:.2f}'.format(weight_for_1))


# Split dataframe in positive and negative
def split_df(df):
  df_pos = df[df['target'] > 0.5]
  df_neg = df[df['target'] < 0.5]
  return df_neg, df_pos


cdf_train_neg, cdf_train_pos = split_df(cdf_train)


def create_ds(panda_dataset):
  filenames = panda_dataset['cough_path']
  targets = panda_dataset['target']
  return tf.data.Dataset.from_tensor_slices((filenames, targets))


train_ds_pos = create_ds(cdf_train_pos).repeat(-1)
train_ds_neg = create_ds(cdf_train_neg).repeat(-1)
val_ds = create_ds(cdf_val)
test_ds = create_ds(cdf_test)


def load_wav_for_map(filename, label):
  return load_wav_16k_mono(filename), label


# Augmentation
def augment_audio(audio, SAMPLE_RATE=16000):
  augmenter = Compose([
    PitchShift(min_semitones=-2, max_semitones=2, p=0.25),
    TimeStretch(min_rate=0.8, max_rate=1.25, p=0.25),
    Shift(min_fraction=-0.1, max_fraction=0.1, p=0.25),
    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.005, p=0.25)
  ])

  return augmenter(samples=audio.numpy(), sample_rate=SAMPLE_RATE)


AUTO = tf.data.experimental.AUTOTUNE

train_ds = tf.data.experimental.sample_from_datasets([train_ds_neg, train_ds_pos], weights=[0.5, 0.5])
train_ds = train_ds.map(load_wav_for_map).map(lambda x, y: (tf.py_function(augment_audio, [x], [tf.float32])[0], y))
val_ds = val_ds.map(load_wav_for_map).map(lambda x, y: (tf.py_function(augment_audio, [x], [tf.float32])[0], y))
test_ds = test_ds.map(load_wav_for_map)


# Notice that we need to define the number of steps per epoch.
# The definition of "epoch" in this case is less clear.
# Say it's the number of batches required to see each positive example once:
BATCH_SIZE = 32
train_steps_per_epoch = np.ceil(2.0 * neg / BATCH_SIZE)


# applies the embedding extraction model to a wav data
def extract_embedding(
        wav_data, label, emb_mean=emb_mean, emb_std=emb_std, max_spec=max_spec, max_mfccs=max_mfccs):
  ''' run YAMNet to extract embedding from the wav data '''
  scores, embeddings, spectrogram = yamnet_model(wav_data)
  # embeddings = tf.squeeze(embeddings, axis=0)
  # embeddings = embeddings - emb_mean
  # embeddings = tf.math.divide(embeddings, emb_std)

  mfccs = tf.signal.mfccs_from_log_mel_spectrograms(spectrogram)[..., 1:39]
  mfccs = (tf.math.divide(mfccs, tf.math.reduce_max(tf.math.abs(mfccs)) * 2) + 0.5) * 255   # values are between [0, 1]
  mfccs = tf.expand_dims(mfccs, axis=-1)
  mfccs = tf.image.grayscale_to_rgb(mfccs)

  spectrogram = (tf.math.divide(spectrogram, max_spec * 2) + 0.5) * 255  # values are between [0, 1]
  spectrogram = tf.expand_dims(spectrogram, axis=-1)
  spectrogram = tf.image.grayscale_to_rgb(spectrogram)

  '''
  if tf.shape(embeddings)[0] > 1:
    embeddings = tf.math.reduce_mean(embeddings, axis=0)
  else:
    embeddings = tf.squeeze(embeddings, axis=0)
  '''
  # ((embeddings, spectrogram, mfccs), label)
  return ((spectrogram, mfccs), label)


# extract embedding
train_ds = train_ds.map(extract_embedding)
val_ds = val_ds.map(extract_embedding)
test_ds = test_ds.map(extract_embedding)
test_ds.element_spec

"""When extracting embeddings from the WAV data, you get an array of shape (N, 1024) where N is the number of frames that YAMNet found (one for every 0.48 seconds of audio)."""

train_ds = train_ds.shuffle(buffer_size=5 * BATCH_SIZE, seed=60).batch(BATCH_SIZE).prefetch(AUTO)
val_ds = val_ds.batch(BATCH_SIZE).prefetch(AUTO)
test_ds = test_ds.cache().batch(BATCH_SIZE).prefetch(AUTO)

'''
# Check a single batch
features, sample_labels = next(iter(train_ds))

emb = features[0]
spec = features[1]
mfccs = features[2]


plt.figure(figsize=(16, 16))
for i, image in enumerate(spec[:9]):
  ax = plt.subplot(3, 3, i + 1)
  plt.imshow(tf.cast(image * 255, dtype=tf.uint8))
  plt.axis("off")
'''

"""Create the model"""

# Resizer architecture with n = 16 and r = 1
# (adapted form https://keras.io/examples/vision/learnable_resizer/)

INTERPOLATION = "bilinear"
TARGET_SIZE = (224, 224)


def conv_block(x, filters, kernel_size, strides, activation=layers.LeakyReLU(0.2)):
  x = layers.Conv2D(filters, kernel_size, strides, padding="same", use_bias=False)(x)
  x = layers.BatchNormalization()(x)
  if activation:
    x = activation(x)
  return x


def res_block(x):
  inputs = x
  x = conv_block(x, 16, 3, 1)
  x = conv_block(x, 16, 3, 1, activation=None)
  return layers.Add()([inputs, x])


def get_learnable_resizer(filters=16, num_res_blocks=1, interpolation=INTERPOLATION):
  inputs = layers.Input(shape=[None, None, 3])

  # First, perform naive resizing.
  naive_resize = layers.experimental.preprocessing.Resizing(
    *TARGET_SIZE, interpolation=interpolation
  )(inputs)

  # First convolution block without batch normalization.
  x = layers.Conv2D(filters=filters, kernel_size=7, strides=1, padding="same")(inputs)
  x = layers.LeakyReLU(0.2)(x)

  # Second convolution block with batch normalization.
  x = layers.Conv2D(filters=filters, kernel_size=1, strides=1, padding="same")(x)
  x = layers.LeakyReLU(0.2)(x)
  x = layers.BatchNormalization()(x)

  # Intermediate resizing as a bottleneck.
  bottleneck = layers.experimental.preprocessing.Resizing(
    *TARGET_SIZE, interpolation=interpolation
  )(x)

  # Residual passes.
  for _ in range(num_res_blocks):
    x = res_block(bottleneck)

  # Projection.
  x = layers.Conv2D(
    filters=filters, kernel_size=3, strides=1, padding="same", use_bias=False
  )(x)
  x = layers.BatchNormalization()(x)

  # Skip connection.
  x = layers.Add()([bottleneck, x])

  # Final resized image.
  x = layers.Conv2D(filters=3, kernel_size=7, strides=1, padding="same")(x)
  final_resize = layers.Add()([naive_resize, x])

  return tf.keras.Model(inputs, final_resize)


learnable_resizer = get_learnable_resizer()


def build_Eff_Net0():
  base_model = tf.keras.applications.EfficientNetB0(include_top=False,
                                                    weights="imagenet",
                                                    drop_connect_rate=0.6)

  # Freeze the pretrained weights
  base_model.trainable = False

  # Compile
  model = Model(inputs=base_model.input, outputs=base_model.outputs)
  return model


EffNet_B0 = build_Eff_Net0()


def unfreeze_model(model):
  # Unfreeze the all layers while leaving BatchNorm layers frozen
  for layer in model.layers[:]:
    if not isinstance(layer, layers.BatchNormalization):
      layer.trainable = True


# unfreeze_model(EffNet_B0)


EffNet_B0.summary()


# @tf.autograph.experimental.do_not_convert
def build_triple_EffNet():

  resizer_1 = get_learnable_resizer()
  resizer_2 = get_learnable_resizer()
  eff_1  = build_Eff_Net0()
  eff_2 = build_Eff_Net0()

  # inputs
  # input_embed = Input(shape=(1024), dtype=tf.float32, name='input_embedding')
  input_spec = Input(shape=(96, 64, 3), name='input_spec')
  input_mfccs = Input(shape=(96, 38, 3), name='input_mfccs')

  '''
  # embeddings model
  x1 = Dense(512, kernel_regularizer=l2(0.01), activity_regularizer=l2(0.001), activation='relu')(input_embed)
  x1 = BatchNormalization(axis=1)(x1)
  x1 = Dropout(0.5)(x1)
  x1 = Dense(256, kernel_regularizer=l2(0.01), activity_regularizer=l2(0.001), activation='relu')(x1)
  x1 = Dropout(0.5)(x1)
  x1 = BatchNormalization(axis=1)(x1)
  '''

  '''
  # mfccs
  x2 = Conv2D(8, kernel_size=(6, 6), activation='relu', padding='same')(input_mfccs)
  x2 = MaxPooling2D(pool_size=(2, 2), padding='same')(x2)
  x2 = Conv2D(16, kernel_size=(3, 3), activation='relu', padding='same')(x2)
  x2 = MaxPooling2D(pool_size=(2, 2), padding='same')(x2)
  x2 = Conv2D(16, kernel_size=(3, 3), activation='relu', padding='same')(x2)
  x2 = MaxPooling2D(pool_size=(2, 2), padding='same')(x2)
  x2 = Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same')(x2)
  x2 = MaxPooling2D(pool_size=(2, 2), padding='same')(x2)
  x2 = Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same')(x2)
  x2 = Flatten()(x2)
  x2 = Dense(64, activation='relu', activity_regularizer=l2(0.001))(x2)
  x2 = BatchNormalization(axis=1)(x2)
  x2 = Dropout(rate=0.5)(x2)
  '''


  # mfccs
  # x2 = resizer_1(input_mfccs)
  x2 = eff_1(input_mfccs)
  out12 = layers.GlobalAveragePooling2D(name="avg_pool2")(x2)
  out12 = BatchNormalization(axis=1)(out12)
  out12 = Dropout(0.5)(out12)

  out22 = layers.GlobalMaxPooling2D(name='Max_pool_2D2')(x2)
  out22 = BatchNormalization(axis=1)(out22)
  out22 = Dropout(0.5)(out22)

  out = concatenate([out12, out22])
  out = BatchNormalization()(out)
  x2 = Dropout(0.5)(out)


  # Spectrogram model
  x3 = resizer_2(input_spec)
  x3 = eff_2(x3)
  out1 = layers.GlobalAveragePooling2D(name="avg_pool")(x3)
  out1 = BatchNormalization(axis=1)(out1)
  out1 = Dropout(0.5)(out1)

  out2 = layers.GlobalMaxPooling2D(name='Max_pool_2D')(x3)
  out2 = BatchNormalization(axis=1)(out2)
  out2 = Dropout(0.5)(out2)

  out2 = concatenate([out1, out2])
  out2 = BatchNormalization()(out2)
  out2 = Dropout(0.5)(out2)

  # concatenate
  # conc = concatenate([x1, x2, out])
  conc = concatenate([out2, out])
  z = Dense(256, activation='relu', activity_regularizer=l2(0.001),
            kernel_regularizer=l2(0.001),
            bias_regularizer=l2(1e-5))(conc)
  z = BatchNormalization(axis=1)(z)
  z = Dropout(0.3)(z)
  z = Dense(1, activation='sigmoid', kernel_regularizer=l2(0.001))(z)

  # model = Model(inputs=[input_embed, input_spec, input_mfccs], outputs=z)
  model = Model(inputs=[input_spec, input_mfccs], outputs=z)
  return model


# my_model = build_model_2D_conv()
# my_model = build_model_LSTM()
# my_model = build_model_TRIPLE()
my_model = build_triple_EffNet()

my_model.summary()

# Monitor validation accuracy during training and save only the best model.
checkpoint_filepath = 'ckpt/triple.ckpt'

# load ckpt after 100 epochs (no_emb)
# my_model.load_weights('ckpt_100_epochs/triple.ckpt')


model_checkpoint = ModelCheckpoint(filepath=checkpoint_filepath,
                                   monitor='val_auc',
                                   verbose=1,
                                   save_weights_only=True,
                                   save_best_only=True,
                                   mode='max')

# Reduce the learning rate by a factor of 0.3 each time the validation accuracy
# does not improve for 10 consecutive epochs.
reduce_lr_loss = ReduceLROnPlateau(monitor='val_auc',
                                   factor=0.3,
                                   patience=25,
                                   verbose=1,
                                   min_delta=1e-4,
                                   cooldown=0,
                                   mode='max')
# Stop the training process if the if validation accuracy does not improve for
# 20 consecutive epochs.
early_stopping = EarlyStopping(monitor='val_auc',
                               verbose=1,
                               patience=120,
                               mode='max',
                               restore_best_weights=True)

# !pip install focal-loss

METRICS = [
  tf.keras.metrics.BinaryAccuracy(name='accuracy'),
  tf.keras.metrics.Precision(name='precision'),
  tf.keras.metrics.Recall(name='recall'),
  tf.keras.metrics.AUC(name='auc'),
  tf.keras.metrics.AUC(name='prc', curve='PR'),  # precision-recall curve
]

my_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),
                 optimizer=tf.keras.optimizers.Adam(learning_rate=5e-4),
                 metrics=METRICS)

EPOCHS = 200

history = my_model.fit(train_ds,
                       callbacks=[model_checkpoint, early_stopping, reduce_lr_loss],
                       validation_data=test_ds,
                       epochs=EPOCHS,
                       steps_per_epoch=train_steps_per_epoch,
                       verbose=1)

# Save weights
my_model.load_weights(checkpoint_filepath)
saved_model_path_weight = 'models/yamnet_covid_DOUBLE_CNN_weight.h5'
my_model.save_weights(saved_model_path_weight)


def plot_acc_loss(history):
  acc = history.history['auc']
  val_acc = history.history['val_auc']

  loss = history.history['loss']
  val_loss = history.history['val_loss']

  plt.figure(figsize=(8, 8))
  plt.subplot(2, 1, 1)
  plt.plot(acc, label='Training auc')
  plt.plot(val_acc, label='Validation auc')
  plt.legend(loc='lower right')
  plt.ylabel('AUC')
  plt.ylim([min(plt.ylim()), 1])
  plt.title('Training and Validation AUC')

  plt.subplot(2, 1, 2)
  plt.plot(loss, label='Training Loss')
  plt.plot(val_loss, label='Validation Loss')
  plt.legend(loc='upper right')
  plt.ylabel('Cross Entropy')
  plt.ylim([0, 1.0])
  plt.title('Training and Validation Loss')
  plt.xlabel('epoch')
  plt.show()


plot_acc_loss(history)


y_pred = my_model.predict(test_ds)
# y_pred = np.argmax(pred, axis=-1) # For softmax
y_true = np.concatenate([y for x, y in test_ds], axis=0)

# ROC
fpr, tpr, thresholds = roc_curve(y_true, y_pred.ravel())

# AUC
AUC = auc(fpr, tpr)

plt.figure()
plt.plot([0, 1], [0, 1], 'k--')
plt.plot(fpr, tpr, label='my_model (area = {:.3f})'.format(AUC))
plt.xlabel('False positive rate')
plt.ylabel('True positive rate')
plt.title('ROC curve')
plt.legend(loc='best')
plt.show()
